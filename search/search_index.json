{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"youtube-history-analysis Documentation : https://armanckeser.github.io/youtube-history-analysis Source Code : https://github.com/armanckeser/youtube-history-analysis PyPI : https://pypi.org/project/youtube-history-analysis/ See how your YouTube interests evolved over time Installation python -m venv yt-history-venv # On Windows ./yt-history-venv/Scripts/activate.bat # On MAC source ./yt-history-venv/bin/activate pip install youtube-history-analysis Usage Get a YouTube API Key Visit the Google Cloud Console . Click the project drop-down and select or create the project for which you want to add an API key. Click the hamburger menu and select APIs & Services > Credentials. On the Credentials page, click Create credentials > API key. The API key created dialog displays your newly created API key. Remember to restrict the API key so that it can only be used with certain websites or IP addresses by clicking the Edit button for the API key and then setting the restrictions in the Key restriction section. Get your YouTube History as JSON Visit Google Takeout and sign in to your Google account. Scroll down to the \"YouTube\" section and click All data included. Click the Deselect All button and then select the checkbox next to Watch history. Click the Next button at the bottom of the page. On the next page, you can select the file type and delivery method for your takeout. Make sure to select JSON as the file type. Click the Create export button to start the export process. Once the export is complete, you will receive an email with a link to download your takeout. The downloaded file will be a zip archive containing your YouTube watch history in JSON format. python -m youtube_history_analysis $API_KEY --watch-history-file-path $WATCH_HISTORY_JSON_PATH This will create an outputs folder with a bunch of .csv files and a few .png files. Feel free to use the .csv file to do your own analysis. Development Clone this repository Requirements: Poetry Python 3.9+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell Testing pytest Documentation The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release. Releasing Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation. Pre-commit Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Introduction"},{"location":"#youtube-history-analysis","text":"Documentation : https://armanckeser.github.io/youtube-history-analysis Source Code : https://github.com/armanckeser/youtube-history-analysis PyPI : https://pypi.org/project/youtube-history-analysis/ See how your YouTube interests evolved over time","title":"youtube-history-analysis"},{"location":"#installation","text":"python -m venv yt-history-venv # On Windows ./yt-history-venv/Scripts/activate.bat # On MAC source ./yt-history-venv/bin/activate pip install youtube-history-analysis","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#get-a-youtube-api-key","text":"Visit the Google Cloud Console . Click the project drop-down and select or create the project for which you want to add an API key. Click the hamburger menu and select APIs & Services > Credentials. On the Credentials page, click Create credentials > API key. The API key created dialog displays your newly created API key. Remember to restrict the API key so that it can only be used with certain websites or IP addresses by clicking the Edit button for the API key and then setting the restrictions in the Key restriction section.","title":"Get a YouTube API Key"},{"location":"#get-your-youtube-history-as-json","text":"Visit Google Takeout and sign in to your Google account. Scroll down to the \"YouTube\" section and click All data included. Click the Deselect All button and then select the checkbox next to Watch history. Click the Next button at the bottom of the page. On the next page, you can select the file type and delivery method for your takeout. Make sure to select JSON as the file type. Click the Create export button to start the export process. Once the export is complete, you will receive an email with a link to download your takeout. The downloaded file will be a zip archive containing your YouTube watch history in JSON format. python -m youtube_history_analysis $API_KEY --watch-history-file-path $WATCH_HISTORY_JSON_PATH This will create an outputs folder with a bunch of .csv files and a few .png files. Feel free to use the .csv file to do your own analysis.","title":"Get your YouTube History as JSON"},{"location":"#development","text":"Clone this repository Requirements: Poetry Python 3.9+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell","title":"Development"},{"location":"#testing","text":"pytest","title":"Testing"},{"location":"#documentation","text":"The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release.","title":"Documentation"},{"location":"#releasing","text":"Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation.","title":"Releasing"},{"location":"#pre-commit","text":"Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Pre-commit"},{"location":"api_docs/","text":"API documentation __main__ main ( youtube_api_key : str , watch_history_file_path : str = WATCH_HISTORY_FILE_PATH ) -> None Given a YT v3 API key and a watch history JSON filepath, creates analysis graphs under outputs folder Parameters: Name Type Description Default youtube_api_key str YouTube api key from https://console.cloud.google.com/apis/dashboard required watch_history_file_path str Path to watch history downloaded from Google Takeout as JSON. Defaults to ./watch-history.json. WATCH_HISTORY_FILE_PATH Source code in youtube_history_analysis/__main__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def main ( youtube_api_key : str , watch_history_file_path : str = WATCH_HISTORY_FILE_PATH ) -> None : \"\"\"Given a YT v3 API key and a watch history JSON filepath, creates analysis graphs under outputs folder Args: youtube_api_key (str): YouTube api key from https://console.cloud.google.com/apis/dashboard watch_history_file_path (str, optional): Path to watch history downloaded from Google Takeout as JSON. Defaults to ./watch-history.json. \"\"\" print ( \"Analysis Started!\" ) print ( \"Connecting to YouTube.\" ) service : YouTubeResource = build ( \"youtube\" , \"v3\" , developerKey = youtube_api_key ) videos_collection = service . videos () video_categories_collection = service . videoCategories () if not os . path . exists ( \"./outputs\" ): os . mkdir ( \"./outputs\" ) print ( \"Loading watch history.\" ) with open ( WATCH_HISTORY_FILE_PATH , encoding = \"utf-8\" ) as watch_history_file : watch_history = json . load ( watch_history_file ) watched_urls = {} for watched in watch_history : url = watched . get ( \"titleUrl\" , None ) title = watched . get ( \"title\" , None )[ 8 :] if url : watched_urls [ title ] = url unique_watched_urls = list ( set ( watched_urls . values ())) unique_watched_ids = [ url [ 32 :] for url in unique_watched_urls ] print ( \"Getting video information from YouTube.\" ) watched_video_snippets = [] for i in range ( 0 , len ( unique_watched_ids ), 50 ): video_collection_request = videos_collection . list ( part = \"snippet\" , id = unique_watched_ids [ i : i + 50 ] # noqa: E203 ) snippets_response = video_collection_request . execute () snippets = [ item [ \"snippet\" ] for item in snippets_response [ \"items\" ]] watched_video_snippets += snippets unique_category_ids = list ( { snippet [ \"categoryId\" ] for snippet in watched_video_snippets } ) video_categories_request = video_categories_collection . list ( part = \"snippet\" , id = unique_category_ids ) video_categories_response = video_categories_request . execute () video_category_id_to_title = {} for item in video_categories_response [ \"items\" ]: video_category_id_to_title [ item [ \"id\" ]] = item [ \"snippet\" ][ \"title\" ] print ( \"Saving video snippets and watch history information to outputs folder.\" ) df_snippets = pd . DataFrame ( watched_video_snippets ) df_watch_history = pd . DataFrame ( watch_history ) df_watch_history . to_csv ( \"outputs/watch_history.csv\" ) df_snippets . to_csv ( \"outputs/snippets.csv\" ) df_watch_history . title = df_watch_history . title . apply ( lambda title : title [ 8 :]) # type: ignore df_merged = df_watch_history . merge ( df_snippets , on = \"title\" , how = \"left\" ) df_merged [ \"categoryTitle\" ] = df_merged [ \"categoryId\" ] . apply ( lambda id : video_category_id_to_title . get ( id , \"NaN\" ) ) df_merged . drop ( [ \"subtitles\" , \"products\" , \"activityControls\" , \"header\" , \"thumbnails\" , \"liveBroadcastContent\" , ], axis = 1 , ) df_merged . to_csv ( \"outputs/full_history.csv\" ) df_merged [ \"time\" ] = pd . to_datetime ( df_merged [ \"time\" ]) # type: ignore df_merged [ \"dayWatched\" ] = pd . to_datetime ( # type: ignore df_merged [ \"time\" ] . apply ( lambda time : str ( time )[: 10 ]) ) df_merged . set_index ( \"title\" ) df_category_to_day = df_merged . filter ( items = [ \"dayWatched\" , \"categoryTitle\" ]) ctdf = ( df_category_to_day . reset_index () . groupby ([ \"dayWatched\" , \"categoryTitle\" ], as_index = False ) . count () # rename isn't strictly necessary here, it's just for readability . rename ( columns = { \"index\" : \"count\" }) ) pivotted = ctdf . pivot_table ( \"count\" , \"dayWatched\" , \"categoryTitle\" ) # type: ignore df_pivotted_by_month = pivotted . resample ( \"M\" ) . sum () percentages = df_pivotted_by_month . div ( df_pivotted_by_month . sum ( axis = 1 ), axis = 0 ) print ( \"Drawing a few example graphs to output folder.\" ) plt . style . use ( \"dark_background\" ) # type: ignore vals = np . linspace ( 0 , 1 , 22 ) np . random . shuffle ( vals ) cmap = plt . cm . colors . ListedColormap ( plt . cm . gnuplot ( vals )) # type: ignore ax = percentages . plot . area ( fontsize = 12 , figsize = ( 20 , 10 ), cmap = cmap ) ax . set_xlabel ( \"Date\" ) ax . set_ylabel ( \"Percentage of Video Watched\" ) ax . set_title ( \"My YouTube Interest by Month\" ) handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], fontsize = 11 , loc = \"upper left\" , bbox_to_anchor = ( 1.0 , 1.0 ), ) ax . get_figure () . savefig ( os . path . join ( \"outputs\" , \"percentages_per_month.png\" )) ax = df_pivotted_by_month . plot . area ( fontsize = 13 , figsize = ( 20 , 10 )) ax . set_xlabel ( \"Date\" ) ax . set_ylabel ( \"Number of videos watched\" ) ax . set_title ( \"My YouTube Interest by Month\" ) handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], fontsize = 11 , loc = \"upper left\" ) ax . get_figure () . savefig ( os . path . join ( \"outputs\" , \"numbers_per_month.png\" )) service . close () print ( \"Done! Go check out your analysis in the outputs folder!\" \"You can also run some on your own using the csv files.\" \"Thanks for using youtube-history-analysis\" )","title":"API documentation"},{"location":"api_docs/#api-documentation","text":"","title":"API documentation"},{"location":"api_docs/#youtube_history_analysis.__main__","text":"","title":"__main__"},{"location":"api_docs/#youtube_history_analysis.__main__.main","text":"Given a YT v3 API key and a watch history JSON filepath, creates analysis graphs under outputs folder Parameters: Name Type Description Default youtube_api_key str YouTube api key from https://console.cloud.google.com/apis/dashboard required watch_history_file_path str Path to watch history downloaded from Google Takeout as JSON. Defaults to ./watch-history.json. WATCH_HISTORY_FILE_PATH Source code in youtube_history_analysis/__main__.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def main ( youtube_api_key : str , watch_history_file_path : str = WATCH_HISTORY_FILE_PATH ) -> None : \"\"\"Given a YT v3 API key and a watch history JSON filepath, creates analysis graphs under outputs folder Args: youtube_api_key (str): YouTube api key from https://console.cloud.google.com/apis/dashboard watch_history_file_path (str, optional): Path to watch history downloaded from Google Takeout as JSON. Defaults to ./watch-history.json. \"\"\" print ( \"Analysis Started!\" ) print ( \"Connecting to YouTube.\" ) service : YouTubeResource = build ( \"youtube\" , \"v3\" , developerKey = youtube_api_key ) videos_collection = service . videos () video_categories_collection = service . videoCategories () if not os . path . exists ( \"./outputs\" ): os . mkdir ( \"./outputs\" ) print ( \"Loading watch history.\" ) with open ( WATCH_HISTORY_FILE_PATH , encoding = \"utf-8\" ) as watch_history_file : watch_history = json . load ( watch_history_file ) watched_urls = {} for watched in watch_history : url = watched . get ( \"titleUrl\" , None ) title = watched . get ( \"title\" , None )[ 8 :] if url : watched_urls [ title ] = url unique_watched_urls = list ( set ( watched_urls . values ())) unique_watched_ids = [ url [ 32 :] for url in unique_watched_urls ] print ( \"Getting video information from YouTube.\" ) watched_video_snippets = [] for i in range ( 0 , len ( unique_watched_ids ), 50 ): video_collection_request = videos_collection . list ( part = \"snippet\" , id = unique_watched_ids [ i : i + 50 ] # noqa: E203 ) snippets_response = video_collection_request . execute () snippets = [ item [ \"snippet\" ] for item in snippets_response [ \"items\" ]] watched_video_snippets += snippets unique_category_ids = list ( { snippet [ \"categoryId\" ] for snippet in watched_video_snippets } ) video_categories_request = video_categories_collection . list ( part = \"snippet\" , id = unique_category_ids ) video_categories_response = video_categories_request . execute () video_category_id_to_title = {} for item in video_categories_response [ \"items\" ]: video_category_id_to_title [ item [ \"id\" ]] = item [ \"snippet\" ][ \"title\" ] print ( \"Saving video snippets and watch history information to outputs folder.\" ) df_snippets = pd . DataFrame ( watched_video_snippets ) df_watch_history = pd . DataFrame ( watch_history ) df_watch_history . to_csv ( \"outputs/watch_history.csv\" ) df_snippets . to_csv ( \"outputs/snippets.csv\" ) df_watch_history . title = df_watch_history . title . apply ( lambda title : title [ 8 :]) # type: ignore df_merged = df_watch_history . merge ( df_snippets , on = \"title\" , how = \"left\" ) df_merged [ \"categoryTitle\" ] = df_merged [ \"categoryId\" ] . apply ( lambda id : video_category_id_to_title . get ( id , \"NaN\" ) ) df_merged . drop ( [ \"subtitles\" , \"products\" , \"activityControls\" , \"header\" , \"thumbnails\" , \"liveBroadcastContent\" , ], axis = 1 , ) df_merged . to_csv ( \"outputs/full_history.csv\" ) df_merged [ \"time\" ] = pd . to_datetime ( df_merged [ \"time\" ]) # type: ignore df_merged [ \"dayWatched\" ] = pd . to_datetime ( # type: ignore df_merged [ \"time\" ] . apply ( lambda time : str ( time )[: 10 ]) ) df_merged . set_index ( \"title\" ) df_category_to_day = df_merged . filter ( items = [ \"dayWatched\" , \"categoryTitle\" ]) ctdf = ( df_category_to_day . reset_index () . groupby ([ \"dayWatched\" , \"categoryTitle\" ], as_index = False ) . count () # rename isn't strictly necessary here, it's just for readability . rename ( columns = { \"index\" : \"count\" }) ) pivotted = ctdf . pivot_table ( \"count\" , \"dayWatched\" , \"categoryTitle\" ) # type: ignore df_pivotted_by_month = pivotted . resample ( \"M\" ) . sum () percentages = df_pivotted_by_month . div ( df_pivotted_by_month . sum ( axis = 1 ), axis = 0 ) print ( \"Drawing a few example graphs to output folder.\" ) plt . style . use ( \"dark_background\" ) # type: ignore vals = np . linspace ( 0 , 1 , 22 ) np . random . shuffle ( vals ) cmap = plt . cm . colors . ListedColormap ( plt . cm . gnuplot ( vals )) # type: ignore ax = percentages . plot . area ( fontsize = 12 , figsize = ( 20 , 10 ), cmap = cmap ) ax . set_xlabel ( \"Date\" ) ax . set_ylabel ( \"Percentage of Video Watched\" ) ax . set_title ( \"My YouTube Interest by Month\" ) handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], fontsize = 11 , loc = \"upper left\" , bbox_to_anchor = ( 1.0 , 1.0 ), ) ax . get_figure () . savefig ( os . path . join ( \"outputs\" , \"percentages_per_month.png\" )) ax = df_pivotted_by_month . plot . area ( fontsize = 13 , figsize = ( 20 , 10 )) ax . set_xlabel ( \"Date\" ) ax . set_ylabel ( \"Number of videos watched\" ) ax . set_title ( \"My YouTube Interest by Month\" ) handles , labels = ax . get_legend_handles_labels () ax . legend ( handles [:: - 1 ], labels [:: - 1 ], fontsize = 11 , loc = \"upper left\" ) ax . get_figure () . savefig ( os . path . join ( \"outputs\" , \"numbers_per_month.png\" )) service . close () print ( \"Done! Go check out your analysis in the outputs folder!\" \"You can also run some on your own using the csv files.\" \"Thanks for using youtube-history-analysis\" )","title":"main()"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased 1.0.2 - 2022-12-09 Added Added prints of script state for better user experience 1.0.1 - 2022-12-08 Changed Changed README.md and added more documentation 1.0.0 - 2022-12-08 Added Added main function that creates a few analysis files Use poetry run python -m youtube_history_analysis $API_KEY $WATCH_HISTORY_FILE_PATH to create analysis","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#102-2022-12-09","text":"","title":"1.0.2 - 2022-12-09"},{"location":"changelog/#added","text":"Added prints of script state for better user experience","title":"Added"},{"location":"changelog/#101-2022-12-08","text":"","title":"1.0.1 - 2022-12-08"},{"location":"changelog/#changed","text":"Changed README.md and added more documentation","title":"Changed"},{"location":"changelog/#100-2022-12-08","text":"","title":"1.0.0 - 2022-12-08"},{"location":"changelog/#added_1","text":"Added main function that creates a few analysis files Use poetry run python -m youtube_history_analysis $API_KEY $WATCH_HISTORY_FILE_PATH to create analysis","title":"Added"}]}